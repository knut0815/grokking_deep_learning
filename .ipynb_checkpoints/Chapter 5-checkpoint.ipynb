{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent learning with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset of size `(3, 4)`: 3 training examples, each with 4 features\n",
    "xs = np.array([[8.50, 9.5, 9.9, 9.0],\n",
    "               [0.65, 0.8, 0.8, 0.9],\n",
    "               [1.20, 1.3, 0.5, 1.0]])\n",
    "\n",
    "ys = np.array([1.0, 1.0, 0.0, 1.0])\n",
    "\n",
    "# A single training example\n",
    "x_i = xs[:, 0]\n",
    "y_i = ys[0]\n",
    "\n",
    "w = np.array([0.1, 0.2, -0.1])\n",
    "\n",
    "# A simple linear model\n",
    "def forward(x):\n",
    "    y_hat = w.dot(x)\n",
    "    return y_hat\n",
    "\n",
    "y_hat = forward(x_i)\n",
    "l = (y_hat - y_i) ** 2.0\n",
    "\n",
    "# Here, we ignore the extra `2.0` scaling factor that would normally\n",
    "# be a part of the \"real\" partial derivative of the loss function with\n",
    "# respect to the weights, since the author doesn't include it\n",
    "dl_dw =  (y_hat - y_i) * x_i # * 2.0?\n",
    "\n",
    "# Our only hyperparameter: the learning rate\n",
    "lr = 0.01\n",
    "\n",
    "w -= lr * dl_dw\n",
    "print(\"Weights:\" + str(w))\n",
    "print(\"Weight Deltas:\" + str(dl_dw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with several steps of learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([[8.50, 9.5, 9.9, 9.0],\n",
    "               [0.65, 0.8, 0.8, 0.9],\n",
    "               [1.20, 1.3, 0.5, 1.0]])\n",
    "\n",
    "ys = np.array([1.0, 1.0, 0.0, 1.0])\n",
    "\n",
    "x_i = xs[:, 0]\n",
    "y_i = ys[0]\n",
    "\n",
    "w = np.array([0.1, 0.2, -0.1])\n",
    "\n",
    "def forward(x, w):\n",
    "    y_hat = w.dot(x)\n",
    "    return y_hat\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 50\n",
    "lr = 0.001\n",
    "\n",
    "for i in range(epochs): \n",
    "    # Forwards\n",
    "    y_hat = forward(x_i, w)\n",
    "\n",
    "    # Calculate loss\n",
    "    l = (y_hat - y_i) ** 2.0\n",
    "\n",
    "    # Backwards\n",
    "    dl_dw =  (y_hat - y_i) * x_i # * 2.0?\n",
    "\n",
    "    # Weight update\n",
    "    w -= lr * dl_dw\n",
    "    print('epoch: {} - loss: {}, prediction: {}'.format(i, l, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing one weight: what does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([[8.50, 9.5, 9.9, 9.0],\n",
    "               [0.65, 0.8, 0.8, 0.9],\n",
    "               [1.20, 1.3, 0.5, 1.0]])\n",
    "\n",
    "ys = np.array([1.0, 1.0, 0.0, 1.0])\n",
    "\n",
    "x_i = xs[:, 0]\n",
    "y_i = ys[0]\n",
    "\n",
    "w = np.array([0.1, 0.2, -0.1])\n",
    "\n",
    "def forward(x, w):\n",
    "    y_hat = w.dot(x)\n",
    "    return y_hat\n",
    "\n",
    "# Hyperparameters: here, we can increase the learning rate,\n",
    "# since we are \"freezing\" `w_0`\n",
    "epochs = 50\n",
    "lr = 0.3\n",
    "\n",
    "for i in range(epochs): \n",
    "    # Forwards\n",
    "    y_hat = forward(x_i, w)\n",
    "\n",
    "    # Calculate loss\n",
    "    l = (y_hat - y_i) ** 2.0\n",
    "\n",
    "    # Backwards\n",
    "    dl_dw =  (y_hat - y_i) * x_i # * 2.0?\n",
    "    \n",
    "    # \"Freeze\" any updates to `w_0`\n",
    "    dl_dw[0] = 0.0\n",
    "    \n",
    "    # Weight update\n",
    "    w -= lr * dl_dw\n",
    "    print('epoch: {} - loss: {}, prediction: {}'.format(i, l, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent learning with multiple outputs and a single input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - loss (total): 1.00115, prediction: [0.195 0.13  0.585]\n",
      "epoch: 1 - loss (total): 0.918339940321875, prediction: [0.19098625 0.1667575  0.56450875]\n",
      "epoch: 2 - loss (total): 0.8423795095543973, prediction: [0.18714208 0.201962   0.54488326]\n",
      "epoch: 3 - loss (total): 0.7727021410703248, prediction: [0.18346033 0.2356791  0.52608694]\n",
      "epoch: 4 - loss (total): 0.7087881317655766, prediction: [0.17993413 0.26797166 0.50808476]\n",
      "epoch: 5 - loss (total): 0.6501607657458451, prediction: [0.17655691 0.29889986 0.49084318]\n",
      "epoch: 6 - loss (total): 0.5963827586422256, prediction: [0.17332238 0.32852134 0.47433006]\n",
      "epoch: 7 - loss (total): 0.5470529960350563, prediction: [0.17022451 0.35689131 0.45851461]\n",
      "epoch: 8 - loss (total): 0.5018035416588289, prediction: [0.16725753 0.38406265 0.44336737]\n",
      "epoch: 9 - loss (total): 0.4602968930732402, prediction: [0.1644159  0.41008601 0.4288601 ]\n",
      "epoch: 10 - loss (total): 0.42222346433124297, prediction: [0.16169432 0.43500987 0.41496576]\n",
      "epoch: 11 - loss (total): 0.3872992768680507, prediction: [0.15908774 0.45888071 0.40165846]\n",
      "epoch: 12 - loss (total): 0.3552638413881146, prediction: [0.15659128 0.481743   0.38891339]\n",
      "epoch: 13 - loss (total): 0.3258782149516869, prediction: [0.1542003  0.50363935 0.3767068 ]\n",
      "epoch: 14 - loss (total): 0.2989232187693466, prediction: [0.15191034 0.52461059 0.36501593]\n",
      "epoch: 15 - loss (total): 0.2741978034115411, prediction: [0.14971713 0.54469579 0.35381901]\n",
      "epoch: 16 - loss (total): 0.25151754923971825, prediction: [0.14761658 0.5639324  0.34309516]\n",
      "epoch: 17 - loss (total): 0.23071329087420178, prediction: [0.14560478 0.58235625 0.33282439]\n",
      "epoch: 18 - loss (total): 0.21162985543912283, prediction: [0.14367798 0.6000017  0.32298756]\n",
      "epoch: 19 - loss (total): 0.1941249051733418, prediction: [0.14183258 0.61690163 0.31356633]\n",
      "epoch: 20 - loss (total): 0.17806787577473543, prediction: [0.14006515 0.63308754 0.30454316]\n",
      "epoch: 21 - loss (total): 0.1633390025592704, prediction: [0.1383724  0.64858959 0.29590121]\n",
      "epoch: 22 - loss (total): 0.14982842717126804, prediction: [0.13675117 0.66343668 0.28762438]\n",
      "epoch: 23 - loss (total): 0.13743537818207321, prediction: [0.13519843 0.67765648 0.27969725]\n",
      "epoch: 24 - loss (total): 0.1260674194654542, prediction: [0.1337113  0.69127549 0.27210504]\n",
      "epoch: 25 - loss (total): 0.1156397607435829, prediction: [0.13228699 0.7043191  0.2648336 ]\n",
      "epoch: 26 - loss (total): 0.10607462516116245, prediction: [0.13092287 0.71681162 0.25786938]\n",
      "epoch: 27 - loss (total): 0.097300669170621, prediction: [0.12961638 0.72877633 0.2511994 ]\n",
      "epoch: 28 - loss (total): 0.08925245040146484, prediction: [0.12836509 0.74023553 0.24481123]\n",
      "epoch: 29 - loss (total): 0.08186993954478582, prediction: [0.12716666 0.75121058 0.23869295]\n",
      "epoch: 30 - loss (total): 0.07509807261221009, prediction: [0.12601887 0.76172193 0.23283318]\n",
      "epoch: 31 - loss (total): 0.0688863402297207, prediction: [0.12491957 0.77178918 0.22722097]\n",
      "epoch: 32 - loss (total): 0.06318841090301562, prediction: [0.12386672 0.78143109 0.22184589]\n",
      "epoch: 33 - loss (total): 0.05796178544444841, prediction: [0.12285835 0.79066562 0.2166979 ]\n",
      "epoch: 34 - loss (total): 0.053167479984022444, prediction: [0.12189259 0.79951    0.21176741]\n",
      "epoch: 35 - loss (total): 0.048769735200111505, prediction: [0.12096762 0.8079807  0.20704524]\n",
      "epoch: 36 - loss (total): 0.044735749601142696, prediction: [0.12008174 0.81609352 0.20252258]\n",
      "epoch: 37 - loss (total): 0.04103543486886854, prediction: [0.11923329 0.82386357 0.198191  ]\n",
      "epoch: 38 - loss (total): 0.037641191438404775, prediction: [0.11842068 0.83130533 0.19404243]\n",
      "epoch: 39 - loss (total): 0.034527702641151584, prediction: [0.11764241 0.83843268 0.19006914]\n",
      "epoch: 40 - loss (total): 0.031671745875170174, prediction: [0.11689702 0.8452589  0.18626372]\n",
      "epoch: 41 - loss (total): 0.029052019394589578, prediction: [0.11618312 0.85179671 0.18261907]\n",
      "epoch: 42 - loss (total): 0.026648983426117306, prediction: [0.11549938 0.8580583  0.17912842]\n",
      "epoch: 43 - loss (total): 0.024444714427587475, prediction: [0.11484453 0.86405534 0.17578524]\n",
      "epoch: 44 - loss (total): 0.022422771401504255, prediction: [0.11421735 0.869799   0.17258332]\n",
      "epoch: 45 - loss (total): 0.020568073266452026, prediction: [0.11361667 0.87529999 0.16951667]\n",
      "epoch: 46 - loss (total): 0.018866786371722025, prediction: [0.11304136 0.88056857 0.16657959]\n",
      "epoch: 47 - loss (total): 0.017306221316159193, prediction: [0.11249037 0.88561455 0.1637666 ]\n",
      "epoch: 48 - loss (total): 0.015874738301631924, prediction: [0.11196265 0.89044733 0.16107247]\n",
      "epoch: 49 - loss (total): 0.014561660315183581, prediction: [0.11145723 0.89507593 0.15849215]\n"
     ]
    }
   ],
   "source": [
    "# Single input, 3 outputs\n",
    "xs = np.array([0.65, 1.0, 1.0, 0.9])\n",
    "ys = np.array([[0.1, 0.0, 0.0, 0.1],\n",
    "               [1.0, 1.0, 0.0, 1.0],\n",
    "               [0.1, 0.0, 0.1, 0.2]])\n",
    "\n",
    "x_i = xs[0]\n",
    "y_i = ys[:, 0]\n",
    "\n",
    "w = np.array([0.3, 0.2, 0.9])\n",
    "\n",
    "def forward(x):\n",
    "    y_hat = w.dot(x)\n",
    "    return y_hat\n",
    "\n",
    "epochs = 50\n",
    "lr = 0.1\n",
    "\n",
    "for i in range(epochs): \n",
    "    # Forwards\n",
    "    y_hat = forward(x_i)         # Shape `(3,)`\n",
    "    \n",
    "    # Calculate loss\n",
    "    l = (y_hat - y_i) ** 2.0     # Shape `(3,)`\n",
    "    l_total = np.sum(l)\n",
    "    \n",
    "    # Backwards\n",
    "    dl_dw =  (y_hat - y_i) * x_i # * 2.0?\n",
    "    \n",
    "    # Weight update\n",
    "    w -= lr * dl_dw\n",
    "    print('epoch: {} - loss (total): {}, prediction: {}'.format(i, l_total, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent learning with multiple inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_i (shape): (3,)\n",
      "y_i (shape): (3,)\n",
      "w (shape): (3, 3)\n",
      "epoch: 0 - loss (total): 0.9556500000000003, prediction: [0.555 0.98  0.965]\n",
      "epoch: 1 - loss (total): 0.8192478418174456, prediction: [0.52127881 0.98148225 0.90089269]\n",
      "epoch: 2 - loss (total): 0.7023146824910186, prediction: [0.49005679 0.98285465 0.84153653]\n",
      "epoch: 3 - loss (total): 0.6020716663082419, prediction: [0.4611487  0.98412533 0.7865794 ]\n",
      "epoch: 4 - loss (total): 0.5161365701276204, prediction: [0.43438307 0.98530184 0.73569529]\n",
      "epoch: 5 - loss (total): 0.4424671910847189, prediction: [0.4096011  0.98639116 0.68858232]\n",
      "epoch: 6 - loss (total): 0.37931281470327316, prediction: [0.38665579 0.98739975 0.64496101]\n",
      "epoch: 7 - loss (total): 0.32517261007623827, prediction: [0.36541102 0.98833358 0.60457259]\n",
      "epoch: 8 - loss (total): 0.27875996339988907, prediction: [0.34574074 0.98919821 0.56717745]\n",
      "epoch: 9 - loss (total): 0.2389719022659645, prediction: [0.32752828 0.98999876 0.53255376]\n",
      "epoch: 10 - loss (total): 0.2048628840960613, prediction: [0.31066559 0.99073997 0.50049612]\n",
      "epoch: 11 - loss (total): 0.1756223258140486, prediction: [0.29505264 0.99142626 0.47081435]\n",
      "epoch: 12 - loss (total): 0.15055534076086385, prediction: [0.2805968  0.99206168 0.44333238]\n",
      "epoch: 13 - loss (total): 0.129066225074481, prediction: [0.26721232 0.99265001 0.41788715]\n",
      "epoch: 14 - loss (total): 0.11064430109746584, prediction: [0.2548198  0.99319473 0.39432774]\n",
      "epoch: 15 - loss (total): 0.0948517813880589, prediction: [0.24334571 0.99369909 0.37251438]\n",
      "epoch: 16 - loss (total): 0.08131336492932287, prediction: [0.232722   0.99416607 0.35231766]\n",
      "epoch: 17 - loss (total): 0.06970731829567528, prediction: [0.22288564 0.99459843 0.33361776]\n",
      "epoch: 18 - loss (total): 0.05975782982536881, prediction: [0.21377828 0.99499876 0.31630377]\n",
      "epoch: 19 - loss (total): 0.051228455099803824, prediction: [0.20534589 0.99536941 0.30027295]\n",
      "epoch: 20 - loss (total): 0.04391649796489952, prediction: [0.19753844 0.9957126  0.28543023]\n",
      "epoch: 21 - loss (total): 0.0376481935624174, prediction: [0.19030962 0.99603035 0.27168753]\n",
      "epoch: 22 - loss (total): 0.03227457889848368, prediction: [0.18361655 0.99632455 0.25896334]\n",
      "epoch: 23 - loss (total): 0.02766795281551785, prediction: [0.17741952 0.99659694 0.24718217]\n",
      "epoch: 24 - loss (total): 0.023718841240642392, prediction: [0.17168177 0.99684915 0.23627413]\n",
      "epoch: 25 - loss (total): 0.020333395591280185, prediction: [0.16636925 0.99708267 0.22617451]\n",
      "epoch: 26 - loss (total): 0.017431162512401663, prediction: [0.16145046 0.99729888 0.2168234 ]\n",
      "epoch: 27 - loss (total): 0.014943171944387823, prediction: [0.15689621 0.99749907 0.20816533]\n",
      "epoch: 28 - loss (total): 0.012810298085435826, prediction: [0.15267949 0.99768442 0.20014893]\n",
      "epoch: 29 - loss (total): 0.010981854297631386, prediction: [0.14877528 0.99785603 0.19272664]\n",
      "epoch: 30 - loss (total): 0.00941438856536195, prediction: [0.14516043 0.99801493 0.18585444]\n",
      "epoch: 31 - loss (total): 0.008070650880765548, prediction: [0.14181347 0.99816205 0.17949155]\n",
      "epoch: 32 - loss (total): 0.006918708016668492, prediction: [0.13871457 0.99829826 0.17360023]\n",
      "epoch: 33 - loss (total): 0.005931184650050469, prediction: [0.13584534 0.99842438 0.16814553]\n",
      "epoch: 34 - loss (total): 0.005084612801731387, prediction: [0.13318875 0.99854115 0.1630951 ]\n",
      "epoch: 35 - loss (total): 0.004358874132052299, prediction: [0.13072905 0.99864927 0.15841896]\n",
      "epoch: 36 - loss (total): 0.0037367218389972733, prediction: [0.12845164 0.99874938 0.15408939]\n",
      "epoch: 37 - loss (total): 0.003203370796913759, prediction: [0.12634302 0.99884207 0.15008069]\n",
      "epoch: 38 - loss (total): 0.0027461461956915373, prediction: [0.12439067 0.99892788 0.14636908]\n",
      "epoch: 39 - loss (total): 0.002354182330492837, prediction: [0.12258302 0.99900734 0.14293255]\n",
      "epoch: 40 - loss (total): 0.0020181643839282514, prediction: [0.12090934 0.99908091 0.13975071]\n",
      "epoch: 41 - loss (total): 0.00173010706426626, prediction: [0.11935969 0.99914902 0.13680469]\n",
      "epoch: 42 - loss (total): 0.0014831648391286007, prediction: [0.1179249  0.99921209 0.134077  ]\n",
      "epoch: 43 - loss (total): 0.0012714692549737087, prediction: [0.11659644 0.99927049 0.13155147]\n",
      "epoch: 44 - loss (total): 0.0010899894763505959, prediction: [0.11536643 0.99932455 0.12921311]\n",
      "epoch: 45 - loss (total): 0.0009344127307109875, prediction: [0.11422759 0.99937461 0.12704806]\n",
      "epoch: 46 - loss (total): 0.0008010418176128537, prediction: [0.11317315 0.99942096 0.12504346]\n",
      "epoch: 47 - loss (total): 0.0006867072466748858, prediction: [0.11219685 0.99946387 0.12318742]\n",
      "epoch: 48 - loss (total): 0.0005886919162860894, prediction: [0.11129291 0.99950361 0.12146895]\n",
      "epoch: 49 - loss (total): 0.0005046665430991971, prediction: [0.11045597 0.9995404  0.11987783]\n"
     ]
    }
   ],
   "source": [
    "# 3 inputs, 3 outputs\n",
    "xs = np.array([[8.50, 9.5, 9.9, 9.0],  # Toes\n",
    "               [0.65, 0.8, 0.8, 0.9],  # Win-loss record\n",
    "               [1.20, 1.3, 0.5, 1.0]]) # Number of fans (millions)\n",
    "\n",
    "ys = np.array([[0.1, 0.0, 0.0, 0.1],  # Hurt\n",
    "               [1.0, 1.0, 0.0, 1.0],  # Win\n",
    "               [0.1, 0.0, 0.1, 0.2]]) # Sad\n",
    "\n",
    "x_i = xs[:, 0]\n",
    "y_i = ys[:, 0]\n",
    "print('x_i (shape): {}'.format(x_i.shape))\n",
    "print('y_i (shape): {}'.format(y_i.shape))\n",
    "\n",
    "w = np.array([[0.1, 0.1, -0.3],  # Hurt?\n",
    "              [0.1, 0.2,  0.0],  # Win?\n",
    "              [0.0, 1.3,  0.1]]) # Sad?\n",
    "print('w (shape): {}'.format(w.shape))\n",
    "\n",
    "def forward(x):\n",
    "    y_hat = w.dot(x)\n",
    "    return y_hat\n",
    "\n",
    "epochs = 50\n",
    "lr = 0.001\n",
    "\n",
    "for i in range(epochs): \n",
    "    # Forwards:\n",
    "    # y_hat[0] -> hurt?\n",
    "    # y_hat[1] -> win?\n",
    "    # y_hat[2] -> sad?\n",
    "    y_hat = forward(x_i)         # Shape `(3,)`\n",
    "    \n",
    "    # Calculate loss\n",
    "    l = (y_hat - y_i) ** 2.0     # Shape `(3,)`\n",
    "    l_total = np.sum(l)\n",
    "    \n",
    "    # Backwards\n",
    "    dl_dw =  np.outer((y_hat - y_i), x_i) # * 2.0?\n",
    "    \n",
    "    # Weight update\n",
    "    w -= lr * dl_dw\n",
    "    print('epoch: {} - loss (total): {}, prediction: {}'.format(i, l_total, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the outer product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a (shape): (3,)\n",
      "b (shape): (3,)\n",
      "c (shape): (3, 3)\n",
      "[[ 0  0  0]\n",
      " [ 3  4  5]\n",
      " [ 6  8 10]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0, 1, 2])\n",
    "b = np.array([3, 4, 5])\n",
    "print('a (shape): {}'.format(a.shape))\n",
    "print('b (shape): {}'.format(b.shape))\n",
    "\n",
    "c = np.outer(a, b)\n",
    "print('c (shape): {}'.format(c.shape)) # Should be `(3, 3)`\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
